{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_1Yn5KyrE_U",
        "outputId": "0773b7eb-e43a-46ee-e32c-472d38d1348a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jd3spQT-rE_V",
        "outputId": "dbb8cfd8-99cb-4b76-ea83-d88e8efd26de",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: slimit in /usr/local/lib/python3.10/dist-packages (0.8.1)\n",
            "Requirement already satisfied: ply>=3.4 in /usr/local/lib/python3.10/dist-packages (from slimit) (3.11)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\n",
            "Requirement already satisfied: fastBPE in /usr/local/lib/python3.10/dist-packages (0.1.0)\n",
            "Requirement already satisfied: javalang in /usr/local/lib/python3.10/dist-packages (0.13.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from javalang) (1.16.0)\n",
            "Requirement already satisfied: submitit in /usr/local/lib/python3.10/dist-packages (1.4.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from submitit) (2.2.1)\n",
            "Requirement already satisfied: typing_extensions>=3.7.4.2 in /usr/local/lib/python3.10/dist-packages (from submitit) (4.7.1)\n",
            "Requirement already satisfied: dpu-utils in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
            "Requirement already satisfied: azure-storage-blob in /usr/local/lib/python3.10/dist-packages (from dpu-utils) (12.17.0)\n",
            "Requirement already satisfied: azure-identity in /usr/local/lib/python3.10/dist-packages (from dpu-utils) (1.14.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from dpu-utils) (1.23.5)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.10/dist-packages (from dpu-utils) (0.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dpu-utils) (4.66.1)\n",
            "Requirement already satisfied: SetSimilaritySearch in /usr/local/lib/python3.10/dist-packages (from dpu-utils) (1.0.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from dpu-utils) (0.1.99)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from dpu-utils) (1.15.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from dpu-utils) (2023.6.3)\n",
            "Requirement already satisfied: azure-core<2.0.0,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from azure-identity->dpu-utils) (1.29.3)\n",
            "Requirement already satisfied: cryptography>=2.5 in /usr/local/lib/python3.10/dist-packages (from azure-identity->dpu-utils) (41.0.3)\n",
            "Requirement already satisfied: msal<2.0.0,>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from azure-identity->dpu-utils) (1.23.0)\n",
            "Requirement already satisfied: msal-extensions<2.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from azure-identity->dpu-utils) (1.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from azure-storage-blob->dpu-utils) (4.7.1)\n",
            "Requirement already satisfied: isodate>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from azure-storage-blob->dpu-utils) (0.6.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->dpu-utils) (2.21)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.10/dist-packages (from azure-core<2.0.0,>=1.11.0->azure-identity->dpu-utils) (2.31.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from azure-core<2.0.0,>=1.11.0->azure-identity->dpu-utils) (1.16.0)\n",
            "Requirement already satisfied: PyJWT[crypto]<3,>=1.0.0 in /usr/lib/python3/dist-packages (from msal<2.0.0,>=1.20.0->azure-identity->dpu-utils) (2.3.0)\n",
            "Requirement already satisfied: portalocker<3,>=1.0 in /usr/local/lib/python3.10/dist-packages (from msal-extensions<2.0.0,>=0.3.0->azure-identity->dpu-utils) (2.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.11.0->azure-identity->dpu-utils) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.11.0->azure-identity->dpu-utils) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.11.0->azure-identity->dpu-utils) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.11.0->azure-identity->dpu-utils) (2023.7.22)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.10/dist-packages (1.6.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse) (0.41.2)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from astunparse) (1.16.0)\n",
            "Requirement already satisfied: stringcase in /usr/local/lib/python3.10/dist-packages (1.2.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.32.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: fairseq in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.15.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq) (0.29.36)\n",
            "Requirement already satisfied: hydra-core<1.1,>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.0.7)\n",
            "Requirement already satisfied: omegaconf<2.1 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.0.6)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq) (2023.6.3)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.3.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq) (4.66.1)\n",
            "Requirement already satisfied: bitarray in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.8.1)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.0.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.23.5)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.10/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq) (4.8)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (4.7.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (2.7.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (4.9.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->fairseq) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->fairseq) (16.0.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq) (2.21)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fairseq) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq) (1.3.0)\n",
            "Requirement already satisfied: tree_sitter in /usr/local/lib/python3.10/dist-packages (0.20.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.12.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.57.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.4.4)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.23.5)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.3.7)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.41.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.10/dist-packages (2.6.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.1)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2.7.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2023.6.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.23.5)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.3)\n",
            "Requirement already satisfied: simpletransformers in /usr/local/lib/python3.10/dist-packages (0.64.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (4.66.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2023.6.3)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (4.32.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2.14.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.2.2)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.2.2)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2.12.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.5.3)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (0.13.3)\n",
            "Requirement already satisfied: wandb>=0.10.32 in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (0.15.8)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.26.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (0.1.99)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (0.16.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (6.0.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (0.3.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (3.1.32)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (1.29.2)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (0.4.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (0.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (1.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (3.20.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (2023.7.22)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (0.3.7)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (3.8.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->simpletransformers) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->simpletransformers) (2023.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->simpletransformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->simpletransformers) (3.2.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit->simpletransformers) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (5.3.1)\n",
            "Requirement already satisfied: importlib-metadata<7,>=1.4 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (6.8.0)\n",
            "Requirement already satisfied: pillow<10,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (9.4.0)\n",
            "Requirement already satisfied: pympler<2,>=0.9 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (1.0.1)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (13.5.2)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (8.2.3)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (4.7.1)\n",
            "Requirement already satisfied: tzlocal<5,>=1.1 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (4.3.1)\n",
            "Requirement already satisfied: validators<1,>=0.2 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (0.21.2)\n",
            "Requirement already satisfied: pydeck<1,>=0.8 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (0.8.0)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (6.3.2)\n",
            "Requirement already satisfied: watchdog>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (3.0.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.57.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (3.4.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (2.3.7)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (0.41.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (3.1.2)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (4.19.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (0.12.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.10.32->simpletransformers) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (1.3.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers) (4.0.10)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->simpletransformers) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7,>=1.4->streamlit->simpletransformers) (3.16.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (2.16.1)\n",
            "Requirement already satisfied: pytz-deprecation-shim in /usr/local/lib/python3.10/dist-packages (from tzlocal<5,>=1.1->streamlit->simpletransformers) (0.1.0.post0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->simpletransformers) (2.1.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers) (5.0.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.9.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->simpletransformers) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->simpletransformers) (3.2.2)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.10/dist-packages (from pytz-deprecation-shim->tzlocal<5,>=1.1->streamlit->simpletransformers) (2023.3)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.2)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.1+cu118)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.1)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install six\n",
        "!pip install numpy\n",
        "!pip install slimit\n",
        "!pip install psutil\n",
        "!pip install fastBPE\n",
        "!pip install javalang\n",
        "!pip install submitit\n",
        "!pip install dpu-utils\n",
        "!pip install astunparse\n",
        "!pip install stringcase\n",
        "!pip install scikit-learn\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install fairseq\n",
        "!pip install tree_sitter\n",
        "!pip install tensorboard\n",
        "!pip install sentencepiece\n",
        "!pip install tensorboardX\n",
        "!pip install sacrebleu\n",
        "!pip install simpletransformers\n",
        "!pip install --upgrade gdown\n",
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MngCNWflrE_W"
      },
      "outputs": [],
      "source": [
        "# general\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import warnings\n",
        "from typing import List,Tuple\n",
        "\n",
        "# ML\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import pandas as pd\n",
        "\n",
        "# visual\n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from tabulate import tabulate\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "\n",
        "# DL\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# HF\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertForSequenceClassification\n",
        "from torchmetrics.text import BLEUScore, MatchErrorRate, CharErrorRate\n",
        "from torchmetrics.text.rouge import ROUGEScore\n",
        "\n",
        "\n",
        "\n",
        "import gzip\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import f1_score\n",
        "from tokenizers.implementations.byte_level_bpe import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.tensorboard.writer import SummaryWriter\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
        "from transformers.data.metrics import acc_and_f1, simple_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frliw6ICrE_W"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLLD4CSV2Xh8"
      },
      "source": [
        "# XLCoST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeXkhVWTMQmo"
      },
      "outputs": [],
      "source": [
        "# !gdown \"14SEj8Q1oQaogQsaBn-9iTlMN334mW3d1&confirm=t\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMGxU8ky6lu0",
        "outputId": "960598c8-e986-4741-8c63-5c27c614b8d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1tZfsYQgWmc2gG340ru5VbrZ5aLIZ41_6&confirm=t\n",
            "To: /content/XLCoST_data.zip\n",
            "100% 298M/298M [00:02<00:00, 136MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown \"1tZfsYQgWmc2gG340ru5VbrZ5aLIZ41_6&confirm=t\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4ljI33AUvcr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "we0gl_b43ZDI",
        "scrolled": true,
        "outputId": "8782973e-7545-47d8-b11c-572a46f10477"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/XLCoST_data.zip\n",
            "replace __MACOSX/._XLCoST_data? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!unzip \"/content/XLCoST_data.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmGwjPrWzC5c",
        "outputId": "c53c147d-1daf-4bba-a99c-39fdedbb3b01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1a_JpJmlaaI7opFYpThZ45aiDtXcUET6A&confirm=t\n",
            "To: /content/train-match-pairs-data-P-J-C#-C-JS.json\n",
            "100% 151M/151M [00:00<00:00, 188MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown \"1a_JpJmlaaI7opFYpThZ45aiDtXcUET6A&confirm=t\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SbIX25lOzjmv",
        "outputId": "c09b56a1-4a05-4454-d064-ad27de5180ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1F3t1jx7oodsJgO0z9CWagUCc4-rZOMR2&confirm=t\n",
            "To: /content/val-match-pairs-data-P-J-C#-C-JS.json\n",
            "\r  0% 0.00/8.25M [00:00<?, ?B/s]\r100% 8.25M/8.25M [00:00<00:00, 131MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown \"1F3t1jx7oodsJgO0z9CWagUCc4-rZOMR2&confirm=t\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "H5lfGCLQ0VYO",
        "outputId": "1f259dbf-d933-4ac8-ae6b-1ee544bfd4a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1_kjMMHSCRBFdmRJryKFmPTnCu9r4NaPa&confirm=t\n",
            "To: /content/val-homogeneous-file-pairs-data-P-J-C#-C-JS.json\n",
            "\r  0% 0.00/8.25M [00:00<?, ?B/s]\r100% 8.25M/8.25M [00:00<00:00, 83.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown \"1_kjMMHSCRBFdmRJryKFmPTnCu9r4NaPa&confirm=t\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Eowc3qDN0ihs",
        "outputId": "bf167591-b152-4505-eb8a-ae6c2163a287"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1uQo7tYkazrYSv16pfDvnrp2-5sOavLwV&confirm=t\n",
            "To: /content/train-homogeneous-file-pairs-data-P-J-C#-C-JS.json\n",
            "100% 151M/151M [00:00<00:00, 176MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown \"1uQo7tYkazrYSv16pfDvnrp2-5sOavLwV&confirm=t\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0lGOnRTAeC2-",
        "outputId": "e594457d-e244-4316-b39e-a44d407d98cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1L1WUBl6m-ce8xQBHTjutbIleDk3DtpE-&confirm=t\n",
            "To: /content/DeepSCC-RoBERTa-model-full-mixed-lang-snips.pt\n",
            "100% 500M/500M [00:03<00:00, 144MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown \"1L1WUBl6m-ce8xQBHTjutbIleDk3DtpE-&confirm=t\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6nGbeOxGfG86",
        "outputId": "86f19fa6-2bf6-414a-f567-db2e7d086ad3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1d_dUfziicRoSQVR9sSv7YYyytL2keJol&confirm=t\n",
            "To: /content/python_classifier_model_end_train.pt\n",
            "100% 334M/334M [00:01<00:00, 179MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown \"1d_dUfziicRoSQVR9sSv7YYyytL2keJol&confirm=t\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3UgZO5bym1ZG",
        "outputId": "2294e5e8-e5a6-42bf-9fc0-dea0dd2835da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1aqybKA-_hT8iZWq1N2YQWWbMrfzgZEEM&confirm=t\n",
            "To: /content/merges.txt\n",
            "\r  0% 0.00/483k [00:00<?, ?B/s]\r100% 483k/483k [00:00<00:00, 130MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown \"1aqybKA-_hT8iZWq1N2YQWWbMrfzgZEEM&confirm=t\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EYyHPtKXm7KD",
        "outputId": "8512df09-202d-4652-f453-564fdfa28ddc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1XTz9dMBZFtWOlryqORfO69DCA0nwEmEA&confirm=t\n",
            "To: /content/vocab.json\n",
            "\r  0% 0.00/994k [00:00<?, ?B/s]\r100% 994k/994k [00:00<00:00, 166MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown \"1XTz9dMBZFtWOlryqORfO69DCA0nwEmEA&confirm=t\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxswqFzU2XwT"
      },
      "source": [
        "# Shared code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylb11_8t2dUf"
      },
      "source": [
        "## Datasets classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cXrTRpqZBs2c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from collections import defaultdict\n",
        "\n",
        "class CodeSnipPairsDataset(Dataset):\n",
        "    def __init__(self, snip_pairs):\n",
        "      self.snip_pairs = snip_pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.snip_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.snip_pairs[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ltzNdamcP_sE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from collections import defaultdict\n",
        "\n",
        "class CodePairsDataset(Dataset):\n",
        "    def __init__(self, java_exmp, python_exmp):\n",
        "      self.java_samples = []\n",
        "      self.python_samples = []\n",
        "      self.samples_idx = []\n",
        "      for k in java_exmp.keys():\n",
        "        self.java_samples.append(' '.join(java_exmp[k]))\n",
        "        self.python_samples.append(' '.join(python_exmp[k]))\n",
        "        self.samples_idx.append(k)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples_idx)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        java_code = self.java_samples[idx]\n",
        "        python_code = self.python_samples[idx]\n",
        "        exmp_id = self.samples_idx[idx]\n",
        "        return exmp_id, java_code, python_code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "F4gzC1GY-Uh0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from collections import defaultdict\n",
        "\n",
        "class MatchCodePairsDataset(Dataset):\n",
        "    def __init__(self, java_exmp, python_exmp):\n",
        "      self.code_pairs = []\n",
        "      rng = np.random.default_rng()\n",
        "      for k in java_exmp.keys():\n",
        "        match_pair = (java_exmp[k], python_exmp[k], 1)\n",
        "        keys_to_sample = list(java_exmp.keys()).copy()\n",
        "        keys_to_sample.remove(k)\n",
        "        sample_key = rng.choice(keys_to_sample)\n",
        "        mismatch_pair = (java_exmp[k], python_exmp[sample_key], 0)\n",
        "        self.code_pairs.append(match_pair)\n",
        "        self.code_pairs.append(mismatch_pair)\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.code_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      java_code, python_code, label = self.code_pairs[idx]\n",
        "      return java_code, python_code, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T5726ALACTK2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from collections import defaultdict\n",
        "\n",
        "class MatchCodeSnipPairsDataset(Dataset):\n",
        "    def __init__(self, snip_pairs, create=True):\n",
        "        if create:\n",
        "          self.snip_pairs = []\n",
        "          rng = np.random.default_rng()\n",
        "          pair_idxs = [ i for i in range(len(snip_pairs))]\n",
        "          for idx, pair in enumerate(snip_pairs):\n",
        "            match_pair = (pair[0], pair[1], 1)\n",
        "            keys_to_sample = pair_idxs.copy()\n",
        "            keys_to_sample.remove(idx)\n",
        "            sample_key = rng.choice(keys_to_sample)\n",
        "            mismatch_pair = (pair[0], snip_pairs[sample_key][1], 0)\n",
        "            self.snip_pairs.append(match_pair)\n",
        "            self.snip_pairs.append(mismatch_pair)\n",
        "        else:\n",
        "            self.snip_pairs = snip_pairs\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.snip_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      java_code, python_code, label = self.snip_pairs[idx]\n",
        "      return java_code, python_code, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1Ccn5M62x-S"
      },
      "source": [
        "## Read data functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "411pA0c-Qq6t"
      },
      "outputs": [],
      "source": [
        "def read_examples(map_filename, code_filename):\n",
        "    \"\"\"Read examples from filename.\"\"\"\n",
        "    examples={}\n",
        "    with open(map_filename) as f1,open(code_filename) as f2:\n",
        "            for line1,line2 in zip(f1,f2):\n",
        "              exp_id = int(line1.strip().split('-')[0])\n",
        "              if exp_id in examples.keys():\n",
        "                examples[exp_id].append(line2.strip())\n",
        "              else:\n",
        "                examples[exp_id] = [line2.strip()]\n",
        "    return examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KvELaYSuAoFc"
      },
      "outputs": [],
      "source": [
        "def read_snip_examples(src_filename, trg_filename):\n",
        "    \"\"\"Read examples from filename.\"\"\"\n",
        "    examples=[]\n",
        "    with open(src_filename) as f1,open(trg_filename) as f2:\n",
        "            for line1,line2 in zip(f1,f2):\n",
        "              examples.append((line1.strip(), line2.strip()))\n",
        "    return examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pyVpQzr43dW5"
      },
      "outputs": [],
      "source": [
        "def read_match_pairs_json(file_path):\n",
        "  with open(file_path, \"r\") as f:\n",
        "    match_pairs = json.load(f)\n",
        "    match_pairs = [(i['code1'], i['code2'], i['label']) for i in match_pairs]\n",
        "\n",
        "    return match_pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV0s2xfa4KBT"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eVTypnshtahc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "\n",
        "\n",
        "class BERT_Arch(nn.Module):\n",
        "    def __init__(self, bert):\n",
        "        super(BERT_Arch, self).__init__()\n",
        "\n",
        "        self.bert = bert\n",
        "\n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "        # relu activation function\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # dense layer 1\n",
        "        self.fc1 = nn.Linear(768, 512)\n",
        "\n",
        "        # dense layer 2 (Output layer)\n",
        "        self.fc2 = nn.Linear(512, 2)\n",
        "\n",
        "        # softmax activation function\n",
        "        self.softmax = nn.LogSoftmax(dim = 1)\n",
        "\n",
        "    # define the forward pass\n",
        "    def forward(self, input, mask):\n",
        "        # pass the inputs to the model\n",
        "        _, cls_hs = self.bert(input, attention_mask = mask, return_dict=False)\n",
        "\n",
        "        x = self.fc1(cls_hs)\n",
        "\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # output layer\n",
        "        x = self.fc2(x)\n",
        "\n",
        "\n",
        "        # apply softmax activation\n",
        "        x = self.softmax(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCjS1olLL-YL"
      },
      "source": [
        "# Translate code functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xjza1bDZV3ui"
      },
      "source": [
        "### Python classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2h0ApPptVxh_",
        "outputId": "07bc6514-bee9-4934-c2cc-8d2cf1ab281d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at huggingface/CodeBERTa-small-v1 and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(52000, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "CODEBERTA_PRETRAINED = \"huggingface/CodeBERTa-small-v1\"\n",
        "\n",
        "\n",
        "# Set up tokenizer\n",
        "# py_tokenizer = RobertaTokenizer.from_pretrained(CODEBERTA_PRETRAINED)\n",
        "py_tokenizer = ByteLevelBPETokenizer(\"/content/vocab.json\", \"/content/merges.txt\")\n",
        "py_tokenizer._tokenizer.post_processor = BertProcessing(\n",
        "    (\"</s>\", py_tokenizer.token_to_id(\"</s>\")), (\"<s>\", py_tokenizer.token_to_id(\"<s>\")),\n",
        ")\n",
        "py_tokenizer.enable_truncation(max_length=256)\n",
        "\n",
        "\n",
        "py_model = RobertaForSequenceClassification.from_pretrained(CODEBERTA_PRETRAINED, num_labels=2)\n",
        "py_model.load_state_dict(torch.load(\"/content/python_classifier_model_end_train.pt\", map_location=device))\n",
        "py_model = py_model.to(device)\n",
        "py_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAj5AUDtV8Uv"
      },
      "source": [
        "### Logic classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "M-cHkuDwVxiA",
        "outputId": "c79de5b9-237b-4e9e-eb5e-a9c1f38a6f39"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BERT_Arch(\n",
              "  (bert): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): RobertaPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              "  (relu): ReLU()\n",
              "  (fc1): Linear(in_features=768, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
              "  (softmax): LogSoftmax(dim=1)\n",
              ")"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from torch.nn import DataParallel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModel, AutoTokenizer\n",
        "from transformers import AdamW\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "model_pretained_name = \"NTUYG/DeepSCC-RoBERTa\"\n",
        "pt_model = AutoModel.from_pretrained(model_pretained_name)\n",
        "\n",
        "logic_tokenizer = AutoTokenizer.from_pretrained(model_pretained_name)\n",
        "\n",
        "for param in pt_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "logic_model = BERT_Arch(pt_model)\n",
        "logic_model.load_state_dict(torch.load(\"/content/DeepSCC-RoBERTa-model-full-mixed-lang-snips.pt\", map_location=device))\n",
        "logic_model = logic_model.to(device)\n",
        "logic_model.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMSBci-TWAl_"
      },
      "source": [
        "### Data preperation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wy3GiOCZVxiA"
      },
      "outputs": [],
      "source": [
        "def convert_snips_to_logic_features(examples, tokenizer, max_source_length=256, max_target_length=256):\n",
        "    tokens = []\n",
        "    masks = []\n",
        "    cls_token = None\n",
        "    sep_token = None\n",
        "    if tokenizer.cls_token and tokenizer.sep_token:\n",
        "        cls_token = tokenizer.cls_token\n",
        "        sep_token = tokenizer.sep_token\n",
        "    else:\n",
        "        cls_token = tokenizer.bos_token\n",
        "        sep_token = tokenizer.eos_token\n",
        "    for example in examples:\n",
        "        java_code, python_code = example\n",
        "\n",
        "        source_tokens = tokenizer.tokenize(java_code)[:max_source_length-2]\n",
        "        source_tokens =[cls_token]+source_tokens+[sep_token]\n",
        "        source_ids =  tokenizer.convert_tokens_to_ids(source_tokens)\n",
        "        source_mask = [1] * len(source_ids)\n",
        "        padding_length = max_source_length - len(source_ids)\n",
        "        source_ids+=[tokenizer.pad_token_id]*padding_length\n",
        "        source_mask+=[0]*padding_length\n",
        "\n",
        "        target_tokens = tokenizer.tokenize(python_code)[:max_target_length-2]\n",
        "        target_tokens = [cls_token]+target_tokens+[sep_token]\n",
        "        target_ids = tokenizer.convert_tokens_to_ids(target_tokens)\n",
        "        target_mask = [1] * len(target_ids)\n",
        "        padding_length = max_target_length - len(target_ids)\n",
        "        target_ids+=[tokenizer.pad_token_id]*padding_length\n",
        "        target_mask+=[0]*padding_length\n",
        "\n",
        "        src_trg_tokens = source_ids + target_ids\n",
        "        src_trg_mask = source_mask + target_mask\n",
        "\n",
        "        tokens.append(torch.tensor(src_trg_tokens))\n",
        "        masks.append(torch.tensor(src_trg_mask))\n",
        "\n",
        "\n",
        "    return tokens, masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-swmgn85VxiA"
      },
      "outputs": [],
      "source": [
        "def convert_examples_to_features(java_examples, py_examples, tokenizer, max_source_length=256, max_target_length=256, max_example_length=16, stage=None):\n",
        "    features = []\n",
        "    cls_token = None\n",
        "    sep_token = None\n",
        "    src_line_max_len = 0\n",
        "    trg_line_max_len = 0\n",
        "    if tokenizer.cls_token and tokenizer.sep_token:\n",
        "        cls_token = tokenizer.cls_token\n",
        "        sep_token = tokenizer.sep_token\n",
        "    else:\n",
        "        cls_token = tokenizer.bos_token\n",
        "        sep_token = tokenizer.eos_token\n",
        "    for example_id in java_examples.keys():\n",
        "        #source\n",
        "        source = java_examples[example_id][:max_example_length]\n",
        "        source_ids = []\n",
        "        source_masks = []\n",
        "        for line in source:\n",
        "          tok_line = tokenizer.tokenize(line)\n",
        "          if len(tok_line) > src_line_max_len:\n",
        "            src_line_max_len = len(tok_line)\n",
        "          tok_line = tok_line[:max_source_length-2]\n",
        "          tok_line =[cls_token]+tok_line+[sep_token]\n",
        "          tok_line_ids =  tokenizer.convert_tokens_to_ids(tok_line)\n",
        "          tok_line_mask = [1] * (len(tok_line_ids))\n",
        "          padding_length = max_source_length - len(tok_line_ids)\n",
        "          tok_line_ids+=[tokenizer.pad_token_id]*padding_length\n",
        "          tok_line_mask+=[0]*padding_length\n",
        "\n",
        "          source_ids.append(tok_line_ids)\n",
        "          source_masks.append(tok_line_mask)\n",
        "\n",
        "        #target\n",
        "        target = py_examples[example_id][:max_example_length]\n",
        "        t_tokenized_lines = []\n",
        "        if stage==\"test\":\n",
        "            t_tokenized_lines.append(tokenizer.tokenize(\"None\"))\n",
        "        else:\n",
        "            for line in target:\n",
        "              tok_line = tokenizer.tokenize(line)\n",
        "              if len(tok_line) > trg_line_max_len:\n",
        "                trg_line_max_len = len(tok_line)\n",
        "              t_tokenized_lines.append(tok_line[:max_target_length-2])\n",
        "\n",
        "        target_ids = []\n",
        "        target_masks = []\n",
        "        for tok_line in t_tokenized_lines:\n",
        "          tok_line = [cls_token]+tok_line+[sep_token]\n",
        "          tok_line_ids = tokenizer.convert_tokens_to_ids(tok_line)\n",
        "          tok_line_mask = [1]*len(tok_line_ids)\n",
        "          padding_length = max_target_length - len(tok_line_ids)\n",
        "          tok_line_ids += [tokenizer.pad_token_id]*padding_length\n",
        "          tok_line_mask += [0]*padding_length\n",
        "\n",
        "          target_ids.append(tok_line_ids)\n",
        "          target_masks.append(tok_line_mask)\n",
        "\n",
        "\n",
        "        features.append(\n",
        "            (\n",
        "                 example_id,\n",
        "                 torch.tensor(source_ids, dtype=torch.long).to(device),\n",
        "                 torch.tensor(target_ids, dtype=torch.long).to(device),\n",
        "                 torch.tensor(source_masks, dtype=torch.long).to(device),\n",
        "                 torch.tensor(target_masks, dtype=torch.long).to(device)\n",
        "            )\n",
        "        )\n",
        "\n",
        "    print(f\"trg_line_max_len: {trg_line_max_len}\")\n",
        "    print(f\"src_line_max_len: {src_line_max_len}\")\n",
        "    return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL_u40YcWHhs"
      },
      "source": [
        "### Translation model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0QNQMIPqVxiA"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Config, T5ForConditionalGeneration, T5Tokenizer, PLBartConfig, PLBartForConditionalGeneration, PLBartTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fzAuTmrGVxiA"
      },
      "outputs": [],
      "source": [
        "trans_model_name = 'Salesforce/codet5-base'\n",
        "# trans_model_name = 'uclanlp/plbart-python-en_XX'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ilYr85bkhkcK"
      },
      "outputs": [],
      "source": [
        "trans_model_chkp_path = \"/content/drive/MyDrive/runi_nlp/nlp-project/models-eden/codet5-base-trans-80-1-1.pt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tuP1_DMOVxiA"
      },
      "outputs": [],
      "source": [
        "# trans_config = T5Config.from_pretrained(trans_model_name)\n",
        "# trans_tokenizer = PLBartTokenizer.from_pretrained(trans_model_name)\n",
        "# trans_model = PLBartForConditionalGeneration.from_pretrained(trans_model_name)\n",
        "trans_tokenizer = RobertaTokenizer.from_pretrained(trans_model_name)\n",
        "trans_model = T5ForConditionalGeneration.from_pretrained(trans_model_name)\n",
        "trans_model.load_state_dict(torch.load(trans_model_chkp_path, map_location=device))\n",
        "trans_model = trans_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zo_PYNRzWNm5"
      },
      "source": [
        "### Training functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mgzLK8jRVxiA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "import os\n",
        "\n",
        "def train(epochs, examples, val_examples, trans_model, ispy_model, logic_model, trans_tok, ispy_tok, logic_tok, train_dataloader, cross_entropy, optimizer, val_dataloader, max_target_length, beam_size):\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    epoch_losses = {}\n",
        "    # set initial loss to infinite\n",
        "    best_valid_loss = float('inf')\n",
        "    result_name = MODEL_FILE_PREFIX + \".txt\"\n",
        "    epoch_losses_file = MODEL_FILE_PREFIX + \"-epoch_losses\" + \".json\"\n",
        "    f = open(result_name, \"w\")\n",
        "    f.close()\n",
        "    # for each epoch\n",
        "    for epoch in range(epochs):\n",
        "        # train model\n",
        "        train_loss, losses, preds = train_epoch(examples, trans_model, ispy_model, logic_model, trans_tok, ispy_tok, logic_tok, train_dataloader, cross_entropy, optimizer, max_target_length, beam_size)\n",
        "        epoch_losses[str(epoch)] = losses\n",
        "\n",
        "        with open(MODEL_FILE_PREFIX+\"-ep-train-preds-\"+str(epoch)+\".json\", \"w\") as f:\n",
        "            json.dump(preds, f)\n",
        "            del preds\n",
        "\n",
        "        # evaluate model\n",
        "        valid_loss, acc_score, preds = evaluate(val_examples, val_dataloader, trans_model, ispy_model, logic_model, trans_tok, ispy_tok, logic_tok, cross_entropy, max_target_length, beam_size)\n",
        "        with open(MODEL_FILE_PREFIX+\"-ep-eval-preds-\"+str(epoch)+\".json\", \"w\") as f:\n",
        "            json.dump(preds, f)\n",
        "            del preds\n",
        "\n",
        "        # save the best model\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save(trans_model.state_dict(), MODEL_FILE_PREFIX+\".pt\")\n",
        "\n",
        "        # append training and validation loss\n",
        "        train_losses.append(train_loss)\n",
        "        valid_losses.append(valid_loss)\n",
        "\n",
        "#         print(f\"Epoch: {epoch}, Loss: {train_loss}, Val accuracy: {acc_score}, Val loss: {valid_loss}\")\n",
        "        print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "        print(f'Validation Loss: {valid_loss:.3f}')\n",
        "        print(f'Accuracy: {acc_score:.3f}')\n",
        "        with open(result_name, \"a\") as f:\n",
        "            f.writelines(f'{train_loss:.3f} {valid_loss:.3f} {acc_score:.3f}')\n",
        "            f.writelines(\"\\n\")\n",
        "\n",
        "        with open(epoch_losses_file, \"w\") as f:\n",
        "            json.dump(epoch_losses, f)\n",
        "\n",
        "    return train_losses, valid_losses\n",
        "\n",
        "\n",
        "def train_epoch(examples, trans_model, ispy_model, logic_model, trans_tok, ispy_tok, logic_tok, train_dataloader, cross_entropy, optimizer, max_target_length, beam_size):\n",
        "    trans_model.train()\n",
        "    ispy_model.eval()\n",
        "    logic_model.eval()\n",
        "\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "    losses = []\n",
        "    total_preds = []\n",
        "\n",
        "    # iterate over batches\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        exmp_idx,source_ids,target_ids,source_mask,target_mask = batch\n",
        "        source_ids = source_ids.to(device)\n",
        "        source_mask = source_mask.to(device)\n",
        "        target_ids = target_ids.to(device)\n",
        "        target_mask = target_mask.to(device)\n",
        "\n",
        "        trans_model.zero_grad()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        try:\n",
        "            outputs = trans_model(input_ids=source_ids, attention_mask=source_mask,\n",
        "                                  labels=target_ids, decoder_attention_mask=target_mask)\n",
        "            trans_loss = outputs.loss\n",
        "        except Exception as e:\n",
        "            print(exmp_idx)\n",
        "            print(source_ids)\n",
        "            print(target_ids)\n",
        "            raise e\n",
        "\n",
        "        preds = trans_model.generate(source_ids,\n",
        "                                       attention_mask=source_mask,\n",
        "                                       use_cache=True,\n",
        "                                       num_beams=beam_size,\n",
        "                                       early_stopping=False,\n",
        "                                       max_length=max_target_length)\n",
        "        pred_ids = list(preds.cpu().numpy())\n",
        "        p = [trans_tok.decode(id, skip_special_tokens=True, clean_up_tokenization_spaces=False) for id in pred_ids]\n",
        "\n",
        "        tok_pycode = pad_sequence([torch.tensor(ispy_tok.encode(i).ids) for i in p], batch_first=True, padding_value=1)\n",
        "        tok_pycode = tok_pycode.to(device)\n",
        "        is_python_logits = ispy_model(tok_pycode)[0]\n",
        "        is_python_pred = torch.argmax(is_python_logits, dim=1)\n",
        "        # if one line in program isn't valid then all program isn't valid\n",
        "        if not torch.all(is_python_pred):\n",
        "          is_python_pred = torch.zeros(is_python_pred.size(), dtype=torch.long).to(device)\n",
        "\n",
        "        fake_model_py_logits = torch.ones(is_python_logits.size()).to(device)\n",
        "        fake_model_py_logits[:,0] = 0\n",
        "        py_loss = cross_entropy(fake_model_py_logits, is_python_pred)\n",
        "\n",
        "        java_examp = examples[exmp_idx][0]\n",
        "        python_examp = examples[exmp_idx][1]\n",
        "\n",
        "        pairs = list(zip(java_examp,p))\n",
        "        tok_pairs, pairs_masks = convert_snips_to_logic_features(pairs, logic_tok)\n",
        "        tok_pairs = torch.stack(tok_pairs).to(device)\n",
        "        pairs_masks = torch.stack(pairs_masks).to(device)\n",
        "\n",
        "        logic_logits = logic_model(tok_pairs, pairs_masks)\n",
        "        logic_preds = torch.argmax(logic_logits, dim=1)\n",
        "        # if one line in program isn't valid then all program isn't valid\n",
        "        if not torch.all(logic_preds):\n",
        "          logic_preds = torch.zeros(logic_preds.size(), dtype=torch.long).to(device)\n",
        "\n",
        "        fake_model_logic_logits = torch.ones(logic_logits.size()).to(device)\n",
        "        fake_model_logic_logits[:,0] = 0\n",
        "        logic_loss = cross_entropy(fake_model_logic_logits, logic_preds)\n",
        "\n",
        "        batch_loss = TRANS_LOSS_PRC*trans_loss + PY_LOSS_PRC*py_loss + LOGIC_LOSS_PRC*logic_loss\n",
        "        curr_loss = batch_loss.item()\n",
        "\n",
        "        if step % 10 == 0 and not step == 0:\n",
        "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "            print(f\"curr_loss: {curr_loss}\")\n",
        "\n",
        "        if step % 100 == 0 and not step == 0:\n",
        "            total_preds.extend((java_examp,python_examp,p))\n",
        "\n",
        "        if step % 400 == 0 and not step == 0 and curr_loss < losses[-1]:\n",
        "            torch.save(trans_model.state_dict(), MODEL_FILE_PREFIX+\".pt\")\n",
        "\n",
        "        # backward pass to calculate the gradients\n",
        "        batch_loss.backward()\n",
        "\n",
        "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "        torch.nn.utils.clip_grad_norm_(trans_model.parameters(), 1.0)\n",
        "\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += curr_loss\n",
        "        losses.append(curr_loss)\n",
        "\n",
        "    # compute the training loss of the epoch\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "    return avg_loss, losses,  [{'java':p[0], 'python_t':p[1], 'python_p':p[2]} for p in total_preds]\n",
        "\n",
        "\n",
        "\n",
        "# function for evaluating the model\n",
        "def evaluate(examples, val_dataloader, trans_model, ispy_model, logic_model, trans_tok, ispy_tok, logic_tok, cross_entropy, max_target_length, beam_size):\n",
        "    print(\"\\nEvaluating...\")\n",
        "\n",
        "    # deactivate dropout layers\n",
        "    trans_model.eval()\n",
        "    ispy_model.eval()\n",
        "    logic_model.eval()\n",
        "\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "    acc_score = 0\n",
        "    total_preds = []\n",
        "    t0 = time.time()\n",
        "    # iterate over batches\n",
        "    for step, batch in enumerate(val_dataloader):\n",
        "\n",
        "        # Progress update every 50 batches.\n",
        "        if step % 50 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = time.time() - t0\n",
        "\n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "        exmp_idx,source_ids,target_ids,source_mask,target_mask = batch\n",
        "        source_ids = source_ids.to(device)\n",
        "        source_mask = source_mask.to(device)\n",
        "        target_ids = target_ids.to(device)\n",
        "        target_mask = target_mask.to(device)\n",
        "\n",
        "\n",
        "        # deactivate autograd\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # model predictions\n",
        "            outputs = trans_model(input_ids=source_ids, attention_mask=source_mask,\n",
        "                                  labels=target_ids, decoder_attention_mask=target_mask)\n",
        "            trans_loss = outputs.loss\n",
        "\n",
        "            preds = trans_model.generate(source_ids,\n",
        "                                           attention_mask=source_mask,\n",
        "                                           use_cache=True,\n",
        "                                           num_beams=beam_size,\n",
        "                                           early_stopping=False,\n",
        "                                           max_length=max_target_length)\n",
        "            pred_ids = list(preds.cpu().numpy())\n",
        "            p = [trans_tok.decode(id, skip_special_tokens=True, clean_up_tokenization_spaces=False) for id in pred_ids]\n",
        "\n",
        "            tok_pycode = pad_sequence([torch.tensor(ispy_tok.encode(i).ids) for i in p], batch_first=True, padding_value=1)\n",
        "            tok_pycode = tok_pycode.to(device)\n",
        "            is_python_logits = ispy_model(tok_pycode)[0]\n",
        "            # if one line in program isn't valid then all program isn't valid\n",
        "            if not torch.all(is_python_pred):\n",
        "              is_python_pred = torch.zeros(is_python_pred.size(), dtype=torch.long).to(device)\n",
        "\n",
        "            fake_model_py_logits = torch.ones(is_python_logits.size()).to(device)\n",
        "            fake_model_py_logits[:,0] = 0\n",
        "            py_loss = cross_entropy(fake_model_py_logits, is_python_pred)\n",
        "\n",
        "            java_examp = examples[exmp_idx][0]\n",
        "            python_examp = examples[exmp_idx][1]\n",
        "            total_preds.extend((java_examp,python_examp,p))\n",
        "\n",
        "            pairs = list(zip(java_examp,p))\n",
        "            tok_pairs, pairs_masks = convert_snips_to_logic_features(pairs, logic_tok)\n",
        "            tok_pairs = torch.stack(tok_pairs).to(device)\n",
        "            pairs_masks = torch.stack(pairs_masks).to(device)\n",
        "\n",
        "            logic_logits = logic_model(tok_pairs, pairs_masks)\n",
        "            # if one line in program isn't valid then all program isn't valid\n",
        "            if not torch.all(logic_preds):\n",
        "              logic_preds = torch.zeros(logic_preds.size(), dtype=torch.long).to(device)\n",
        "\n",
        "            fake_model_logic_logits = torch.ones(logic_logits.size()).to(device)\n",
        "            fake_model_logic_logits[:,0] = 0\n",
        "            logic_loss = cross_entropy(fake_model_logic_logits, logic_preds)\n",
        "\n",
        "            batch_loss = TRANS_LOSS_PRC*trans_loss + PY_LOSS_PRC*py_loss + LOGIC_LOSS_PRC*logic_loss\n",
        "\n",
        "            curr_loss = batch_loss.item()\n",
        "            total_loss += curr_loss\n",
        "\n",
        "    # compute the validation loss of the epoch\n",
        "    for pred in total_preds:\n",
        "          equal_py_lines = [pred[1][i] == pred[2][i] for i in range(len(pred[2]))]\n",
        "          if all(equal_py_lines):\n",
        "                acc_score += 1\n",
        "    avg_loss = total_loss / len(val_dataloader)\n",
        "    acc_score = acc_score / len(examples)\n",
        "\n",
        "    return avg_loss, acc_score, [{'java':p[0], 'python_t':p[1], 'python_p':p[2]} for p in total_preds]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQs-beA2WaCQ"
      },
      "source": [
        "### Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "66N3yiikVxiA",
        "outputId": "66355652-da39-455c-c3b5-6c6d6ff9429a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trg_line_max_len: 639\n",
            "src_line_max_len: 598\n",
            "trg_line_max_len: 670\n",
            "src_line_max_len: 646\n"
          ]
        }
      ],
      "source": [
        "train_java_code_exp = read_examples('/content/XLCoST_data/generation/pair_data_tok_1/Java-Python/train-Java-map.jsonl', '/content/XLCoST_data/generation/pair_data_tok_1/Java-Python/train-Java-Python-tok.java')\n",
        "train_py_code_exp = read_examples('/content/XLCoST_data/generation/pair_data_tok_1/Java-Python/train-Python-map.jsonl', '/content/XLCoST_data/generation/pair_data_tok_1/Java-Python/train-Java-Python-tok.py')\n",
        "train_java_py_exp = convert_examples_to_features(train_java_code_exp, train_py_code_exp, trans_tokenizer)\n",
        "# train_snip_pairs_jp_dataset = CodeSnipPairsDataset(convert_examples_to_features(train_snip_pairs_jp, trans_tokenizer))\n",
        "\n",
        "val_java_code_exp = read_examples('/content/XLCoST_data/generation/pair_data_tok_1/Java-Python/val-Java-map.jsonl', '/content/XLCoST_data/generation/pair_data_tok_1/Java-Python/val-Java-Python-tok.java')\n",
        "val_py_code_exp = read_examples('/content/XLCoST_data/generation/pair_data_tok_1/Java-Python/val-Python-map.jsonl', '/content/XLCoST_data/generation/pair_data_tok_1/Java-Python/val-Java-Python-tok.py')\n",
        "val_java_py_exp = convert_examples_to_features(val_java_code_exp, val_py_code_exp, trans_tokenizer)\n",
        "# val_snip_pairs_jp_dataset = CodeSnipPairsDataset(convert_examples_to_features(val_snip_pairs_jp, trans_tokenizer))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_YRfhhOqlHgh"
      },
      "outputs": [],
      "source": [
        "train_snip_pairs_jp_dataset = train_java_py_exp\n",
        "val_snip_pairs_jp_dataset = val_java_py_exp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LJhnTYLuzHfx"
      },
      "outputs": [],
      "source": [
        "train_java_py_examples = {}\n",
        "for exmp_id in train_java_code_exp.keys():\n",
        "  train_java_py_examples[exmp_id] = (train_java_code_exp[exmp_id], train_py_code_exp[exmp_id])\n",
        "\n",
        "val_java_py_examples = {}\n",
        "for exmp_id in val_java_code_exp.keys():\n",
        "  val_java_py_examples[exmp_id] = (val_java_code_exp[exmp_id], val_py_code_exp[exmp_id])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-6loL6BWc9l"
      },
      "source": [
        "### Traning parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nnhnniIUVxiA"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(trans_model.parameters(), lr = 1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BYTtFV1yVxiA"
      },
      "outputs": [],
      "source": [
        "cross_entropy = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nCPfl7p_VxiA"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "srEur-8GVxiA",
        "outputId": "fcb98ab6-4195-40c6-e170-a291ae7b22da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f945caae650>"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.autograd.set_detect_anomaly(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mGlUe3ZiVxiB"
      },
      "outputs": [],
      "source": [
        "TRANS_LOSS_PRC = 0.8\n",
        "PY_LOSS_PRC = 0.1\n",
        "LOGIC_LOSS_PRC = 0.1\n",
        "MODEL_FILE_PREFIX = \"/content/drive/MyDrive/runi_nlp/nlp-project/models-eden/codet5-small-trans-functions-80-10-10\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3b6ZrF9WhKz"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0SVBA96lVxiB",
        "scrolled": true,
        "outputId": "4f196ac4-51a5-4fb4-e9c3-cd9fdf3dee2d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Batch    10  of  9,151.\n",
            "curr_loss: 0.0733306035399437\n",
            "  Batch    20  of  9,151.\n",
            "curr_loss: 0.07708097994327545\n",
            "  Batch    30  of  9,151.\n",
            "curr_loss: 0.06628087162971497\n",
            "  Batch    40  of  9,151.\n",
            "curr_loss: 0.1908757984638214\n",
            "  Batch    50  of  9,151.\n",
            "curr_loss: 0.06941157579421997\n",
            "  Batch    60  of  9,151.\n",
            "curr_loss: 0.17509067058563232\n",
            "  Batch    70  of  9,151.\n",
            "curr_loss: 0.06590315699577332\n",
            "  Batch    80  of  9,151.\n",
            "curr_loss: 0.06936515867710114\n",
            "  Batch    90  of  9,151.\n",
            "curr_loss: 0.18618321418762207\n",
            "  Batch   100  of  9,151.\n",
            "curr_loss: 0.1776157021522522\n",
            "  Batch   110  of  9,151.\n",
            "curr_loss: 0.0983513668179512\n",
            "  Batch   120  of  9,151.\n",
            "curr_loss: 0.17683149874210358\n",
            "  Batch   130  of  9,151.\n",
            "curr_loss: 0.215836763381958\n",
            "  Batch   140  of  9,151.\n",
            "curr_loss: 0.22601869702339172\n",
            "  Batch   150  of  9,151.\n",
            "curr_loss: 0.1830243021249771\n",
            "  Batch   160  of  9,151.\n",
            "curr_loss: 0.07979828119277954\n",
            "  Batch   170  of  9,151.\n",
            "curr_loss: 0.1809365153312683\n",
            "  Batch   180  of  9,151.\n",
            "curr_loss: 0.1742192506790161\n",
            "  Batch   190  of  9,151.\n",
            "curr_loss: 0.07901956140995026\n",
            "  Batch   200  of  9,151.\n",
            "curr_loss: 0.1310599446296692\n",
            "  Batch   210  of  9,151.\n",
            "curr_loss: 0.07349911332130432\n",
            "  Batch   220  of  9,151.\n",
            "curr_loss: 0.16777893900871277\n",
            "  Batch   230  of  9,151.\n",
            "curr_loss: 0.1847587525844574\n",
            "  Batch   240  of  9,151.\n",
            "curr_loss: 0.18376177549362183\n",
            "  Batch   250  of  9,151.\n",
            "curr_loss: 0.07278705388307571\n",
            "  Batch   260  of  9,151.\n",
            "curr_loss: 0.06866314262151718\n",
            "  Batch   270  of  9,151.\n",
            "curr_loss: 0.08624817430973053\n",
            "  Batch   280  of  9,151.\n",
            "curr_loss: 0.06787150353193283\n",
            "  Batch   290  of  9,151.\n",
            "curr_loss: 0.16875489056110382\n",
            "  Batch   300  of  9,151.\n",
            "curr_loss: 0.0780191719532013\n",
            "  Batch   310  of  9,151.\n",
            "curr_loss: 0.07228294014930725\n",
            "  Batch   320  of  9,151.\n",
            "curr_loss: 0.20093466341495514\n",
            "  Batch   330  of  9,151.\n",
            "curr_loss: 0.1770303100347519\n",
            "  Batch   340  of  9,151.\n",
            "curr_loss: 0.17315790057182312\n",
            "  Batch   350  of  9,151.\n",
            "curr_loss: 0.06687836349010468\n",
            "  Batch   360  of  9,151.\n",
            "curr_loss: 0.2063809186220169\n",
            "  Batch   370  of  9,151.\n",
            "curr_loss: 0.07003931701183319\n",
            "  Batch   380  of  9,151.\n",
            "curr_loss: 0.177413672208786\n",
            "  Batch   390  of  9,151.\n",
            "curr_loss: 0.1763303428888321\n",
            "  Batch   400  of  9,151.\n",
            "curr_loss: 0.07879406213760376\n",
            "  Batch   410  of  9,151.\n",
            "curr_loss: 0.17133480310440063\n",
            "  Batch   420  of  9,151.\n",
            "curr_loss: 0.07799514383077621\n",
            "  Batch   430  of  9,151.\n",
            "curr_loss: 0.18130174279212952\n",
            "  Batch   440  of  9,151.\n",
            "curr_loss: 0.06511427462100983\n",
            "  Batch   450  of  9,151.\n",
            "curr_loss: 0.06569976359605789\n",
            "  Batch   460  of  9,151.\n",
            "curr_loss: 0.07825509458780289\n",
            "  Batch   470  of  9,151.\n",
            "curr_loss: 0.09107907116413116\n",
            "  Batch   480  of  9,151.\n",
            "curr_loss: 0.06789331138134003\n",
            "  Batch   490  of  9,151.\n",
            "curr_loss: 0.0707242488861084\n",
            "  Batch   500  of  9,151.\n",
            "curr_loss: 0.17786189913749695\n",
            "  Batch   510  of  9,151.\n",
            "curr_loss: 0.08263657987117767\n",
            "  Batch   520  of  9,151.\n",
            "curr_loss: 0.17873403429985046\n",
            "  Batch   530  of  9,151.\n",
            "curr_loss: 0.19133837521076202\n",
            "  Batch   540  of  9,151.\n",
            "curr_loss: 0.09574787318706512\n",
            "  Batch   550  of  9,151.\n",
            "curr_loss: 0.22238068282604218\n",
            "  Batch   560  of  9,151.\n",
            "curr_loss: 0.07291816174983978\n",
            "  Batch   570  of  9,151.\n",
            "curr_loss: 0.2080395370721817\n",
            "  Batch   580  of  9,151.\n",
            "curr_loss: 0.13536453247070312\n",
            "  Batch   590  of  9,151.\n",
            "curr_loss: 0.09230147302150726\n",
            "  Batch   600  of  9,151.\n",
            "curr_loss: 0.09198227524757385\n",
            "  Batch   610  of  9,151.\n",
            "curr_loss: 0.0805583968758583\n",
            "  Batch   620  of  9,151.\n",
            "curr_loss: 0.08574380725622177\n",
            "  Batch   630  of  9,151.\n",
            "curr_loss: 0.08743058145046234\n",
            "  Batch   640  of  9,151.\n",
            "curr_loss: 0.08673220872879028\n",
            "  Batch   650  of  9,151.\n",
            "curr_loss: 0.1751425862312317\n",
            "  Batch   660  of  9,151.\n",
            "curr_loss: 0.20664864778518677\n",
            "  Batch   670  of  9,151.\n",
            "curr_loss: 0.10253079235553741\n",
            "  Batch   680  of  9,151.\n",
            "curr_loss: 0.06793317943811417\n",
            "  Batch   690  of  9,151.\n",
            "curr_loss: 0.17118217051029205\n",
            "  Batch   700  of  9,151.\n",
            "curr_loss: 0.0703999251127243\n",
            "  Batch   710  of  9,151.\n",
            "curr_loss: 0.17026878893375397\n",
            "  Batch   720  of  9,151.\n",
            "curr_loss: 0.16959545016288757\n",
            "  Batch   730  of  9,151.\n",
            "curr_loss: 0.175764262676239\n",
            "  Batch   740  of  9,151.\n",
            "curr_loss: 0.1953437775373459\n",
            "  Batch   750  of  9,151.\n",
            "curr_loss: 0.19951686263084412\n",
            "  Batch   760  of  9,151.\n",
            "curr_loss: 0.08471120893955231\n",
            "  Batch   770  of  9,151.\n",
            "curr_loss: 0.06500056385993958\n",
            "  Batch   780  of  9,151.\n",
            "curr_loss: 0.16760864853858948\n",
            "  Batch   790  of  9,151.\n",
            "curr_loss: 0.09285949170589447\n",
            "  Batch   800  of  9,151.\n",
            "curr_loss: 0.15392646193504333\n",
            "  Batch   810  of  9,151.\n",
            "curr_loss: 0.21919387578964233\n",
            "  Batch   820  of  9,151.\n",
            "curr_loss: 0.20606687664985657\n",
            "  Batch   830  of  9,151.\n",
            "curr_loss: 0.22572456300258636\n",
            "  Batch   840  of  9,151.\n",
            "curr_loss: 0.18673576414585114\n",
            "  Batch   850  of  9,151.\n",
            "curr_loss: 0.18401804566383362\n",
            "  Batch   860  of  9,151.\n",
            "curr_loss: 0.06925724446773529\n",
            "  Batch   870  of  9,151.\n",
            "curr_loss: 0.0947384238243103\n",
            "  Batch   880  of  9,151.\n",
            "curr_loss: 0.07246801257133484\n",
            "  Batch   890  of  9,151.\n",
            "curr_loss: 0.07567895948886871\n",
            "  Batch   900  of  9,151.\n",
            "curr_loss: 0.07728306949138641\n",
            "  Batch   910  of  9,151.\n",
            "curr_loss: 0.17616385221481323\n",
            "  Batch   920  of  9,151.\n",
            "curr_loss: 0.17810091376304626\n",
            "  Batch   930  of  9,151.\n",
            "curr_loss: 0.10711631178855896\n",
            "  Batch   940  of  9,151.\n",
            "curr_loss: 0.09241137653589249\n",
            "  Batch   950  of  9,151.\n",
            "curr_loss: 0.09371854364871979\n",
            "  Batch   960  of  9,151.\n",
            "curr_loss: 0.17153960466384888\n",
            "  Batch   970  of  9,151.\n",
            "curr_loss: 0.076531320810318\n",
            "  Batch   980  of  9,151.\n",
            "curr_loss: 0.18154969811439514\n",
            "  Batch   990  of  9,151.\n",
            "curr_loss: 0.075580894947052\n",
            "  Batch 1,000  of  9,151.\n",
            "curr_loss: 0.06408096104860306\n",
            "  Batch 1,010  of  9,151.\n",
            "curr_loss: 0.0743388682603836\n",
            "  Batch 1,020  of  9,151.\n",
            "curr_loss: 0.16447429358959198\n",
            "  Batch 1,030  of  9,151.\n",
            "curr_loss: 0.16470108926296234\n",
            "  Batch 1,040  of  9,151.\n",
            "curr_loss: 0.07545903325080872\n",
            "  Batch 1,050  of  9,151.\n",
            "curr_loss: 0.16738368570804596\n",
            "  Batch 1,060  of  9,151.\n",
            "curr_loss: 0.2537771165370941\n",
            "  Batch 1,070  of  9,151.\n",
            "curr_loss: 0.18473105132579803\n",
            "  Batch 1,080  of  9,151.\n",
            "curr_loss: 0.17660857737064362\n",
            "  Batch 1,090  of  9,151.\n",
            "curr_loss: 0.08770239353179932\n",
            "  Batch 1,100  of  9,151.\n",
            "curr_loss: 0.07957315444946289\n",
            "  Batch 1,110  of  9,151.\n",
            "curr_loss: 0.07062456011772156\n",
            "  Batch 1,120  of  9,151.\n",
            "curr_loss: 0.07867428660392761\n",
            "  Batch 1,130  of  9,151.\n",
            "curr_loss: 0.08910165727138519\n",
            "  Batch 1,140  of  9,151.\n",
            "curr_loss: 0.2543323040008545\n",
            "  Batch 1,150  of  9,151.\n",
            "curr_loss: 0.15536662936210632\n",
            "  Batch 1,160  of  9,151.\n",
            "curr_loss: 0.11486516892910004\n",
            "  Batch 1,170  of  9,151.\n",
            "curr_loss: 0.09111618995666504\n",
            "  Batch 1,180  of  9,151.\n",
            "curr_loss: 0.06379898637533188\n",
            "  Batch 1,190  of  9,151.\n",
            "curr_loss: 0.17289060354232788\n",
            "  Batch 1,200  of  9,151.\n",
            "curr_loss: 0.17403200268745422\n",
            "  Batch 1,210  of  9,151.\n",
            "curr_loss: 0.0666796863079071\n",
            "  Batch 1,220  of  9,151.\n",
            "curr_loss: 0.17392975091934204\n",
            "  Batch 1,230  of  9,151.\n",
            "curr_loss: 0.06606560200452805\n",
            "  Batch 1,240  of  9,151.\n",
            "curr_loss: 0.20153796672821045\n",
            "  Batch 1,250  of  9,151.\n",
            "curr_loss: 0.08575813472270966\n",
            "  Batch 1,260  of  9,151.\n",
            "curr_loss: 0.17688661813735962\n",
            "  Batch 1,270  of  9,151.\n",
            "curr_loss: 0.17800460755825043\n",
            "  Batch 1,280  of  9,151.\n",
            "curr_loss: 0.22626549005508423\n",
            "  Batch 1,290  of  9,151.\n",
            "curr_loss: 0.1804346740245819\n",
            "  Batch 1,300  of  9,151.\n",
            "curr_loss: 0.17558623850345612\n",
            "  Batch 1,310  of  9,151.\n",
            "curr_loss: 0.17427577078342438\n",
            "  Batch 1,320  of  9,151.\n",
            "curr_loss: 0.1075071394443512\n",
            "  Batch 1,330  of  9,151.\n",
            "curr_loss: 0.17771518230438232\n",
            "  Batch 1,340  of  9,151.\n",
            "curr_loss: 0.08182960748672485\n",
            "  Batch 1,350  of  9,151.\n",
            "curr_loss: 0.19137699902057648\n",
            "  Batch 1,360  of  9,151.\n",
            "curr_loss: 0.17124225199222565\n",
            "  Batch 1,370  of  9,151.\n",
            "curr_loss: 0.183625727891922\n",
            "  Batch 1,380  of  9,151.\n",
            "curr_loss: 0.1296408772468567\n",
            "  Batch 1,390  of  9,151.\n",
            "curr_loss: 0.1691817343235016\n",
            "  Batch 1,400  of  9,151.\n",
            "curr_loss: 0.1917712539434433\n",
            "  Batch 1,410  of  9,151.\n",
            "curr_loss: 0.06545504927635193\n",
            "  Batch 1,420  of  9,151.\n",
            "curr_loss: 0.08388414978981018\n",
            "  Batch 1,430  of  9,151.\n",
            "curr_loss: 0.08093541860580444\n",
            "  Batch 1,440  of  9,151.\n",
            "curr_loss: 0.1733781397342682\n",
            "  Batch 1,450  of  9,151.\n",
            "curr_loss: 0.10995537042617798\n",
            "  Batch 1,460  of  9,151.\n",
            "curr_loss: 0.2136925309896469\n",
            "  Batch 1,470  of  9,151.\n",
            "curr_loss: 0.19850040972232819\n",
            "  Batch 1,480  of  9,151.\n",
            "curr_loss: 0.1148083508014679\n",
            "  Batch 1,490  of  9,151.\n",
            "curr_loss: 0.18460384011268616\n",
            "  Batch 1,500  of  9,151.\n",
            "curr_loss: 0.17352817952632904\n",
            "  Batch 1,510  of  9,151.\n",
            "curr_loss: 0.17223751544952393\n",
            "  Batch 1,520  of  9,151.\n",
            "curr_loss: 0.21914872527122498\n",
            "  Batch 1,530  of  9,151.\n",
            "curr_loss: 0.22409547865390778\n",
            "  Batch 1,540  of  9,151.\n",
            "curr_loss: 0.09345816820859909\n",
            "  Batch 1,550  of  9,151.\n",
            "curr_loss: 0.07443181425333023\n",
            "  Batch 1,560  of  9,151.\n",
            "curr_loss: 0.2995912730693817\n",
            "  Batch 1,570  of  9,151.\n",
            "curr_loss: 0.0746467188000679\n",
            "  Batch 1,580  of  9,151.\n",
            "curr_loss: 0.07254800200462341\n",
            "  Batch 1,590  of  9,151.\n",
            "curr_loss: 0.073711097240448\n",
            "  Batch 1,600  of  9,151.\n",
            "curr_loss: 0.08171511441469193\n",
            "  Batch 1,610  of  9,151.\n",
            "curr_loss: 0.08276529610157013\n",
            "  Batch 1,620  of  9,151.\n",
            "curr_loss: 0.07364892959594727\n",
            "  Batch 1,630  of  9,151.\n",
            "curr_loss: 0.2030380368232727\n",
            "  Batch 1,640  of  9,151.\n",
            "curr_loss: 0.18468743562698364\n",
            "  Batch 1,650  of  9,151.\n",
            "curr_loss: 0.21581706404685974\n",
            "  Batch 1,660  of  9,151.\n",
            "curr_loss: 0.17098836600780487\n",
            "  Batch 1,670  of  9,151.\n",
            "curr_loss: 0.17282363772392273\n",
            "  Batch 1,680  of  9,151.\n",
            "curr_loss: 0.07538885623216629\n",
            "  Batch 1,690  of  9,151.\n",
            "curr_loss: 0.16577143967151642\n",
            "  Batch 1,700  of  9,151.\n",
            "curr_loss: 0.1898842751979828\n",
            "  Batch 1,710  of  9,151.\n",
            "curr_loss: 0.06723642349243164\n",
            "  Batch 1,720  of  9,151.\n",
            "curr_loss: 0.21453195810317993\n",
            "  Batch 1,730  of  9,151.\n",
            "curr_loss: 0.18451723456382751\n",
            "  Batch 1,740  of  9,151.\n",
            "curr_loss: 0.07903309911489487\n",
            "  Batch 1,750  of  9,151.\n",
            "curr_loss: 0.07388389110565186\n",
            "  Batch 1,760  of  9,151.\n",
            "curr_loss: 0.08075636625289917\n",
            "  Batch 1,770  of  9,151.\n",
            "curr_loss: 0.0692468136548996\n",
            "  Batch 1,780  of  9,151.\n",
            "curr_loss: 0.07624046504497528\n",
            "  Batch 1,790  of  9,151.\n",
            "curr_loss: 0.10413138568401337\n",
            "  Batch 1,800  of  9,151.\n",
            "curr_loss: 0.07194951176643372\n",
            "  Batch 1,810  of  9,151.\n",
            "curr_loss: 0.06730562448501587\n",
            "  Batch 1,820  of  9,151.\n",
            "curr_loss: 0.1650158315896988\n",
            "  Batch 1,830  of  9,151.\n",
            "curr_loss: 0.07852890342473984\n",
            "  Batch 1,840  of  9,151.\n",
            "curr_loss: 0.07258367538452148\n",
            "  Batch 1,850  of  9,151.\n",
            "curr_loss: 0.1809108406305313\n",
            "  Batch 1,860  of  9,151.\n",
            "curr_loss: 0.19130277633666992\n",
            "  Batch 1,870  of  9,151.\n",
            "curr_loss: 0.17375315725803375\n",
            "  Batch 1,880  of  9,151.\n",
            "curr_loss: 0.22078953683376312\n",
            "  Batch 1,890  of  9,151.\n",
            "curr_loss: 0.10262177139520645\n",
            "  Batch 1,900  of  9,151.\n",
            "curr_loss: 0.13296985626220703\n",
            "  Batch 1,910  of  9,151.\n",
            "curr_loss: 0.0975317507982254\n",
            "  Batch 1,920  of  9,151.\n",
            "curr_loss: 0.09232214093208313\n",
            "  Batch 1,930  of  9,151.\n",
            "curr_loss: 0.0834663063287735\n",
            "  Batch 1,940  of  9,151.\n",
            "curr_loss: 0.1793212890625\n",
            "  Batch 1,950  of  9,151.\n",
            "curr_loss: 0.20323187112808228\n",
            "  Batch 1,960  of  9,151.\n",
            "curr_loss: 0.18251816928386688\n",
            "  Batch 1,970  of  9,151.\n",
            "curr_loss: 0.17564398050308228\n",
            "  Batch 1,980  of  9,151.\n",
            "curr_loss: 0.07306794822216034\n",
            "  Batch 1,990  of  9,151.\n",
            "curr_loss: 0.13111063838005066\n",
            "  Batch 2,000  of  9,151.\n",
            "curr_loss: 0.08931244909763336\n",
            "  Batch 2,010  of  9,151.\n",
            "curr_loss: 0.0861934944987297\n",
            "  Batch 2,020  of  9,151.\n",
            "curr_loss: 0.16856397688388824\n",
            "  Batch 2,030  of  9,151.\n",
            "curr_loss: 0.1667664498090744\n",
            "  Batch 2,040  of  9,151.\n",
            "curr_loss: 0.18267519772052765\n",
            "  Batch 2,050  of  9,151.\n",
            "curr_loss: 0.1845848262310028\n",
            "  Batch 2,060  of  9,151.\n",
            "curr_loss: 0.18212294578552246\n",
            "  Batch 2,070  of  9,151.\n",
            "curr_loss: 0.10296769440174103\n",
            "  Batch 2,080  of  9,151.\n",
            "curr_loss: 0.06674496829509735\n",
            "  Batch 2,090  of  9,151.\n",
            "curr_loss: 0.1724684089422226\n",
            "  Batch 2,100  of  9,151.\n",
            "curr_loss: 0.18474474549293518\n",
            "  Batch 2,110  of  9,151.\n",
            "curr_loss: 0.06849169731140137\n",
            "  Batch 2,120  of  9,151.\n",
            "curr_loss: 0.1963084191083908\n",
            "  Batch 2,130  of  9,151.\n",
            "curr_loss: 0.15601450204849243\n",
            "  Batch 2,140  of  9,151.\n",
            "curr_loss: 0.1759248524904251\n",
            "  Batch 2,150  of  9,151.\n",
            "curr_loss: 0.17209848761558533\n",
            "  Batch 2,160  of  9,151.\n",
            "curr_loss: 0.1866782158613205\n",
            "  Batch 2,170  of  9,151.\n",
            "curr_loss: 0.2336999922990799\n",
            "  Batch 2,180  of  9,151.\n",
            "curr_loss: 0.18156319856643677\n",
            "  Batch 2,190  of  9,151.\n",
            "curr_loss: 0.0731053352355957\n",
            "  Batch 2,200  of  9,151.\n",
            "curr_loss: 0.18179096281528473\n",
            "  Batch 2,210  of  9,151.\n",
            "curr_loss: 0.16611677408218384\n",
            "  Batch 2,220  of  9,151.\n",
            "curr_loss: 0.1959524303674698\n",
            "  Batch 2,230  of  9,151.\n",
            "curr_loss: 0.20731547474861145\n",
            "  Batch 2,240  of  9,151.\n",
            "curr_loss: 0.17342375218868256\n",
            "  Batch 2,250  of  9,151.\n",
            "curr_loss: 0.17804042994976044\n",
            "  Batch 2,260  of  9,151.\n",
            "curr_loss: 0.10782177746295929\n",
            "  Batch 2,270  of  9,151.\n",
            "curr_loss: 0.08332221210002899\n",
            "  Batch 2,280  of  9,151.\n",
            "curr_loss: 0.09293033182621002\n",
            "  Batch 2,290  of  9,151.\n",
            "curr_loss: 0.19453762471675873\n",
            "  Batch 2,300  of  9,151.\n",
            "curr_loss: 0.17776522040367126\n",
            "  Batch 2,310  of  9,151.\n",
            "curr_loss: 0.27036863565444946\n",
            "  Batch 2,320  of  9,151.\n",
            "curr_loss: 0.1083216741681099\n",
            "  Batch 2,330  of  9,151.\n",
            "curr_loss: 0.1140466183423996\n",
            "  Batch 2,340  of  9,151.\n",
            "curr_loss: 0.07148496806621552\n",
            "  Batch 2,350  of  9,151.\n",
            "curr_loss: 0.09324462711811066\n",
            "  Batch 2,360  of  9,151.\n",
            "curr_loss: 0.12785279750823975\n",
            "  Batch 2,370  of  9,151.\n",
            "curr_loss: 0.17694585025310516\n",
            "  Batch 2,380  of  9,151.\n",
            "curr_loss: 0.08630996942520142\n",
            "  Batch 2,390  of  9,151.\n",
            "curr_loss: 0.21650531888008118\n",
            "  Batch 2,400  of  9,151.\n",
            "curr_loss: 0.06712950766086578\n",
            "  Batch 2,410  of  9,151.\n",
            "curr_loss: 0.170732781291008\n",
            "  Batch 2,420  of  9,151.\n",
            "curr_loss: 0.41925692558288574\n",
            "  Batch 2,430  of  9,151.\n",
            "curr_loss: 0.09729659557342529\n"
          ]
        }
      ],
      "source": [
        "train_losses, valid_losses = train(epochs=1,\n",
        "                                    examples=train_java_py_examples,\n",
        "                                    val_examples=val_java_py_examples,\n",
        "                                    trans_model=trans_model,\n",
        "                                    ispy_model=py_model,\n",
        "                                    logic_model=logic_model,\n",
        "                                    trans_tok=trans_tokenizer,\n",
        "                                    ispy_tok=py_tokenizer,\n",
        "                                    logic_tok=logic_tokenizer,\n",
        "                                    train_dataloader=train_snip_pairs_jp_dataset,\n",
        "                                    cross_entropy=cross_entropy,\n",
        "                                    optimizer=optimizer,\n",
        "                                    val_dataloader=val_snip_pairs_jp_dataset,\n",
        "                                    max_target_length=256,\n",
        "                                    beam_size=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KK_WXqqRaMz4"
      },
      "outputs": [],
      "source": [
        "plt.title(\"Train Loss\")\n",
        "plt.xlabel(\"Batch number\")\n",
        "plt.ylabel(\"Loss Value\")\n",
        "plt.plot(range(len(train_losses[0])), train_losses[0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuMclTSfxdoD"
      },
      "outputs": [],
      "source": [
        "!pip install codebleu\n",
        "!pip install torchmetrics\n",
        "\n",
        "from codebleu import calc_codebleu\n",
        "from torchmetrics.text import BLEUScore, MatchErrorRate, CharErrorRate, TranslationEditRate, SacreBLEUScore\n",
        "from torchmetrics.text.rouge import ROUGEScore\n",
        "from torchmetrics import MetricCollection\n",
        "\n",
        "with open(MODEL_FILE_PREFIX+\"-ep-train-preds-0.json\", \"r\") as f:\n",
        "    train_preds = json.load(f)\n",
        "\n",
        "with open(MODEL_FILE_PREFIX+\"-ep-eval-preds-0.json\", \"r\") as f:\n",
        "    eval_preds = json.load(f)\n",
        "\n",
        "def compute_metrics(preds):\n",
        "    true_py = [p['python_t'] for p in preds]\n",
        "    pred_py = [p['python_p'] for p in preds]\n",
        "\n",
        "    bleu = BLEUScore()\n",
        "    sbleu = SacreBLEUScore()\n",
        "    ter = TranslationEditRate()\n",
        "\n",
        "    rouge = ROUGEScore()\n",
        "    mer = MatchErrorRate()\n",
        "    cer = CharErrorRate()\n",
        "\n",
        "    metrics_a = MetricCollection(bleu,sbleu,ter)\n",
        "    metrics_b = MetricCollection(rouge,mer,cer)\n",
        "\n",
        "    for p in preds:\n",
        "        metrics_a.update([p['python_p']], [[p['python_t']]])\n",
        "        metrics_b.update(p['python_p'], p['python_t'])\n",
        "\n",
        "    metrics_a.plot(together=True)\n",
        "    metrics_b.plot(together=True)\n",
        "\n",
        "    return metrics_a.compute(), metrics_b.compute(), calc_codebleu(true_py,pred_py, lang=\"python\", weights=(0.25, 0.25, 0.25, 0.25), tokenizer=None)\n",
        "\n",
        "t_m_a, t_m_b, t_codebleu = compute_metrics(train_preds)\n",
        "v_m_a, v_m_b, v_codebleu = compute_metrics(eval_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DA-TjOM6O0ld"
      },
      "outputs": [],
      "source": [
        "print(v_m_a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25OXbEusO2o9"
      },
      "outputs": [],
      "source": [
        "print(v_m_b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7og3lLOO4mm"
      },
      "outputs": [],
      "source": [
        "print(v_codebleu)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}